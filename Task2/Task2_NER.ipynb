{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e06cfa-44c2-474c-83f0-c97deb2284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9e66c-ba04-4999-8142-d75e3541b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#Load pre-trained model and tokenizer\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check the original label mappings from the pre-trained model\n",
    "original_id2label = model.config.id2label\n",
    "original_label2id = model.config.label2id\n",
    "#print(\"Original labels:\", original_id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b927b955-3187-463b-b8c7-9befaa691503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 2600/2600 [16:07<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.00042790482715964536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████| 650/650 [00:43<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.11852206967749217\n",
      "Validation Accuracy: 0.9907\n",
      "Validation Precision: 0.9916\n",
      "Validation Recall: 0.9907\n",
      "Validation F1: 0.9909\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 2600/2600 [15:40<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.1891512759868524e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████| 650/650 [00:42<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.12479027056866439\n",
      "Validation Accuracy: 0.9907\n",
      "Validation Precision: 0.9916\n",
      "Validation Recall: 0.9907\n",
      "Validation F1: 0.9909\n",
      "Text: There is a lion in the picture.\n",
      "Detected animals: ['lion']\n",
      "\n",
      "Text: The image shows a beautiful dolphin jumping.\n",
      "Detected animals: ['dolphin']\n",
      "\n",
      "Text: I can see both a giraffe and a zebra in this photo.\n",
      "Detected animals: ['giraffe', 'zebra']\n",
      "\n",
      "Text: This appears to be a picture of a small rabbit.\n",
      "Detected animals: ['rabbit.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(text, word_labels, tokenizer, label_map, max_len=128, label_all_tokens=False):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    encoding = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            if word_idx != previous_word_idx:\n",
    "                labels.append(label_map.get(word_labels[word_idx], label_map[\"O\"]))\n",
    "            else:\n",
    "\n",
    "                if label_all_tokens:\n",
    "                    label = label_map.get(word_labels[word_idx], label_map[\"O\"])\n",
    "                    if label == label_map.get(\"B-ANIMAL\"):\n",
    "                        label = label_map.get(\"I-ANIMAL\")\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "    encoding[\"labels\"] = torch.tensor(labels)\n",
    "    return encoding\n",
    "    \n",
    "\n",
    "class AnimalNERDataset(Dataset):\n",
    "    def __init__(self, texts, tags, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tags = tags \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.label_map = {\n",
    "            \"O\": original_label2id[\"O\"],          # Outside any entity\n",
    "            \"B-ANIMAL\": original_label2id[\"B-MISC\"],# Beginning of animal entity\n",
    "            \"I-ANIMAL\": original_label2id[\"I-MISC\"] # Inside of animal entity\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        word_tags = self.tags[idx]  \n",
    "\n",
    "        encoding = tokenize_and_align_labels(\n",
    "            text, \n",
    "            word_tags, \n",
    "            self.tokenizer, \n",
    "            self.label_map, \n",
    "            max_len=self.max_len, \n",
    "            label_all_tokens=True\n",
    "        )\n",
    "    \n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "def create_sample_data():\n",
    "    df = pd.read_csv(\"/Users/anastasiiaserputko/Test/Task2/animal_sentences.csv\")\n",
    "        \n",
    "    texts = df['sentence'].tolist()\n",
    "\n",
    "    animal_list = [\n",
    "    \"cat\", \"feline\", \"kitty\", \"kitten\", \"tomcat\", \"puss\",\n",
    "    \"bear\", \"grizzly\", \"bruin\", \"cub\", \"ursine\",\n",
    "    \"goose\", \"gander\", \"gosling\", \"waterfowl\",\n",
    "    \"squirrel\", \"chipmunk\", \"tree-dweller\",\n",
    "    \"fox\", \"vixen\", \"reynard\", \"tod\",\n",
    "    \"elk\", \"moose\", \"wapiti\", \"stag\",\n",
    "    \"flamingo\", \"wader\", \"pinkbird\",\n",
    "    \"owl\", \"hooter\",\n",
    "    \"frog\", \"toad\", \"amphibian\", \"croaker\",\n",
    "    \"beaver\", \"dam-builder\",\n",
    "    \"bee\", \"honeybee\", \"bumblebee\", \"drone\",\n",
    "    \"dove\", \"pigeon\", \"columbidae\", \"columbid\",\n",
    "    \"ladybug\", \"ladybird\", \"beetle\", \"coccinellid\"\n",
    "    ]\n",
    "        \n",
    "    tags = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_tags = []\n",
    "        for word in words:\n",
    "            lower_word = word.lower().strip(\".,!?\")\n",
    "            if lower_word in animal_list:\n",
    "                word_tags.append(\"B-ANIMAL\")\n",
    "            else:\n",
    "                word_tags.append(\"O\")\n",
    "        tags.append(word_tags)\n",
    "    \n",
    "    \n",
    "    split_index = int(0.8 * len(texts))  \n",
    "    train_texts = texts[:split_index]\n",
    "    train_tags = tags[:split_index]\n",
    "    val_texts = texts[split_index:]\n",
    "    val_tags = tags[split_index:]\n",
    "    \n",
    "    return train_texts, train_tags, val_texts, val_tags\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, epochs=2):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                pred = torch.argmax(logits, dim=2)\n",
    "                \n",
    "                active_mask = labels != -100\n",
    "                \n",
    "\n",
    "                predictions.extend(pred[active_mask].cpu().numpy())\n",
    "                true_labels.extend(labels[active_mask].cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average=\"weighted\"\n",
    "        )\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        \n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"Validation F1: {f1:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_animals(text, model, tokenizer):\n",
    "    model.eval()\n",
    "    words = text.split()\n",
    "    encoding = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    word_ids = encoding.word_ids(batch_index=0)  # отримуємо індекси слів для кожного токена\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    \n",
    "    offset_mapping = encoding.get(\"offset_mapping\", None)\n",
    "    if offset_mapping is not None:\n",
    "        offset_mapping = offset_mapping.squeeze().tolist()\n",
    "    else:\n",
    "        offset_mapping = [(None, None)] * len(tokens)  # Заглушка, щоб уникнути помилок\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2).squeeze().cpu().numpy()\n",
    "        \n",
    "    #print(f\"\\nText: {text}\")\n",
    "    #print(f\"Tokens: {tokens}\")\n",
    "    #print(f\"Word IDs: {word_ids}\")\n",
    "    #print(f\"Offset Mapping: {offset_mapping}\")\n",
    "    #print(f\"Predictions: {predictions}\")\n",
    "\n",
    "    #print(\"\\nToken Predictions:\")\n",
    "    for token, word_id, offset, pred in zip(tokens, word_ids, offset_mapping, predictions):\n",
    "        label_name = original_id2label.get(pred, \"O\")\n",
    "        #print(f\"Token: {token.ljust(10)} | Word ID: {str(word_id).ljust(3)} | Offset: {offset} | Prediction: {label_name}\")\n",
    "\n",
    "    word_predictions = {}\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        token = tokens[idx]\n",
    "\n",
    "        token_clean = token.replace(\"##\", \"\")\n",
    "        if word_id not in word_predictions:\n",
    "            word_predictions[word_id] = {\"text\": token_clean, \"predictions\": [predictions[idx]]}\n",
    "        else:\n",
    "            word_predictions[word_id][\"text\"] += token_clean\n",
    "            word_predictions[word_id][\"predictions\"].append(predictions[idx])\n",
    "    \n",
    "    results = []\n",
    "    for word_id, info in word_predictions.items():\n",
    "        if info[\"predictions\"][0] == original_label2id[\"B-MISC\"]:\n",
    "            results.append(info[\"text\"])\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main():\n",
    "    # Create sample data\n",
    "    train_texts, train_tags, val_texts, val_tags = create_sample_data()\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = AnimalNERDataset(train_texts, train_tags, tokenizer)\n",
    "    val_dataset = AnimalNERDataset(val_texts, val_tags, tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    fine_tuned_model = train_model(model, train_dataloader, val_dataloader, epochs=2)\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    fine_tuned_model.save_pretrained(\"animal_ner_model\")\n",
    "    tokenizer.save_pretrained(\"animal_ner_model\")\n",
    "    \n",
    "    # Test on sample sentences\n",
    "    test_sentences = [\n",
    "        \"There is a lion in the picture.\",\n",
    "        \"The image shows a beautiful dolphin jumping.\",\n",
    "        \"I can see both a giraffe and a zebra in this photo.\",\n",
    "        \"This appears to be a picture of a small rabbit.\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        animals = extract_animals(sentence, fine_tuned_model, tokenizer)\n",
    "        print(f\"Text: {sentence}\")\n",
    "        print(f\"Detected animals: {animals}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73857b0f-2f41-4a97-a389-9ba84566c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing model on various sentences...\n",
      "\n",
      "Text: The cat is sitting on the windowsill.\n",
      "Detected Animals: ['cat']\n",
      "==================================================\n",
      "Text: The feline stretched lazily in the sun.\n",
      "Detected Animals: ['feline']\n",
      "==================================================\n",
      "Text: A kitten was playing with a ball of yarn.\n",
      "Detected Animals: ['kitten', 'yarn.']\n",
      "==================================================\n",
      "Text: The bear caught a fish from the river.\n",
      "Detected Animals: ['bear']\n",
      "==================================================\n",
      "Text: A large grizzly wandered through the forest.\n",
      "Detected Animals: ['grizzly']\n",
      "==================================================\n",
      "Text: The cub followed its mother closely.\n",
      "Detected Animals: ['cub']\n",
      "==================================================\n",
      "Text: A goose honked loudly near the pond.\n",
      "Detected Animals: ['goose']\n",
      "==================================================\n",
      "Text: The gander led its flock across the field.\n",
      "Detected Animals: ['gander']\n",
      "==================================================\n",
      "Text: A gosling swam behind its mother.\n",
      "Detected Animals: ['gosling']\n",
      "==================================================\n",
      "Text: A squirrel is collecting nuts for winter.\n",
      "Detected Animals: ['squirrel']\n",
      "==================================================\n",
      "Text: The chipmunk darted into its burrow.\n",
      "Detected Animals: ['chipmunk']\n",
      "==================================================\n",
      "Text: A ground squirrel peeked out from behind a tree.\n",
      "Detected Animals: ['squirrel']\n",
      "==================================================\n",
      "Text: The fox ran swiftly through the meadow.\n",
      "Detected Animals: ['fox']\n",
      "==================================================\n",
      "Text: A vixen and her cubs played in the field.\n",
      "Detected Animals: ['vixen', 'cubs']\n",
      "==================================================\n",
      "Text: The red fox is known for its cunning nature.\n",
      "Detected Animals: ['fox']\n",
      "==================================================\n",
      "Text: An elk stood majestically in the clearing.\n",
      "Detected Animals: ['elk']\n",
      "==================================================\n",
      "Text: The stag had an impressive set of antlers.\n",
      "Detected Animals: ['stag', 'antlers.']\n",
      "==================================================\n",
      "Text: A wapiti grazed peacefully in the meadow.\n",
      "Detected Animals: ['wapiti']\n",
      "==================================================\n",
      "Text: A flamingo stood gracefully on one leg.\n",
      "Detected Animals: ['flamingo']\n",
      "==================================================\n",
      "Text: The wader dipped its beak into the water.\n",
      "Detected Animals: ['wader', 'beak']\n",
      "==================================================\n",
      "Text: A bright pinkbird preened its feathers.\n",
      "Detected Animals: []\n",
      "==================================================\n",
      "Text: An owl hooted softly in the night.\n",
      "Detected Animals: ['owl']\n",
      "==================================================\n",
      "Text: The horned owl watched from its perch.\n",
      "Detected Animals: ['owl']\n",
      "==================================================\n",
      "Text: A barn owl soared silently over the field.\n",
      "Detected Animals: ['owl']\n",
      "==================================================\n",
      "Text: The frog jumped into the pond.\n",
      "Detected Animals: ['frog']\n",
      "==================================================\n",
      "Text: A toad sat on a lily pad.\n",
      "Detected Animals: ['toad', 'lily']\n",
      "==================================================\n",
      "Text: The tree frog clung to the branch.\n",
      "Detected Animals: ['frog']\n",
      "==================================================\n",
      "Text: A beaver built a dam in the stream.\n",
      "Detected Animals: ['beaver']\n",
      "==================================================\n",
      "Text: The dam-builder gnawed on a piece of wood.\n",
      "Detected Animals: ['dam-builder']\n",
      "==================================================\n",
      "Text: A muskrat swam near the shore.\n",
      "Detected Animals: ['muskrat']\n",
      "==================================================\n",
      "Text: A bee buzzed around the flowers.\n",
      "Detected Animals: ['bee']\n",
      "==================================================\n",
      "Text: The bumblebee collected nectar from a blossom.\n",
      "Detected Animals: ['bumblebee', 'nectar']\n",
      "==================================================\n",
      "Text: A honeybee returned to the hive with pollen.\n",
      "Detected Animals: ['honeybee', 'hive']\n",
      "==================================================\n",
      "Text: A dove perched on the rooftop.\n",
      "Detected Animals: ['dove']\n",
      "==================================================\n",
      "Text: The pigeon cooed softly in the park.\n",
      "Detected Animals: ['pigeon']\n",
      "==================================================\n",
      "Text: A squab was learning to fly.\n",
      "Detected Animals: ['squab']\n",
      "==================================================\n",
      "Text: A ladybug landed on my hand.\n",
      "Detected Animals: ['ladybug']\n",
      "==================================================\n",
      "Text: The ladybird crawled on a leaf.\n",
      "Detected Animals: ['ladybird']\n",
      "==================================================\n",
      "Text: A little beetle scurried across the petal.A tree-dweller landed on my hand.\n",
      "Detected Animals: ['beetle', 'tree-dweller']\n",
      "==================================================\n",
      "Text: A little spotted bug across the petal.\n",
      "Detected Animals: ['bug', 'petal.']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_model_on_examples(model, tokenizer):\n",
    "    test_sentences = [\n",
    "    \"The cat is sitting on the windowsill.\",\n",
    "    \"The feline stretched lazily in the sun.\",\n",
    "    \"A kitten was playing with a ball of yarn.\",\n",
    "    \n",
    "    \"The bear caught a fish from the river.\",\n",
    "    \"A large grizzly wandered through the forest.\",\n",
    "    \"The cub followed its mother closely.\",\n",
    "    \n",
    "    \"A goose honked loudly near the pond.\",\n",
    "    \"The gander led its flock across the field.\",\n",
    "    \"A gosling swam behind its mother.\",\n",
    "    \n",
    "    \"A squirrel is collecting nuts for winter.\",\n",
    "    \"The chipmunk darted into its burrow.\",\n",
    "    \"A ground squirrel peeked out from behind a tree.\",\n",
    "    \n",
    "    \"The fox ran swiftly through the meadow.\",\n",
    "    \"A vixen and her cubs played in the field.\",\n",
    "    \"The red fox is known for its cunning nature.\",\n",
    "    \n",
    "    \"An elk stood majestically in the clearing.\",\n",
    "    \"The stag had an impressive set of antlers.\",\n",
    "    \"A wapiti grazed peacefully in the meadow.\",\n",
    "    \n",
    "    \"A flamingo stood gracefully on one leg.\",\n",
    "    \"The wader dipped its beak into the water.\",\n",
    "    \"A bright pinkbird preened its feathers.\",\n",
    "    \n",
    "    \"An owl hooted softly in the night.\",\n",
    "    \"The horned owl watched from its perch.\",\n",
    "    \"A barn owl soared silently over the field.\",\n",
    "    \n",
    "    \"The frog jumped into the pond.\",\n",
    "    \"A toad sat on a lily pad.\",\n",
    "    \"The tree frog clung to the branch.\",\n",
    "    \n",
    "    \"A beaver built a dam in the stream.\",\n",
    "    \"The dam-builder gnawed on a piece of wood.\",\n",
    "    \"A muskrat swam near the shore.\",\n",
    "    \n",
    "    \"A bee buzzed around the flowers.\",\n",
    "    \"The bumblebee collected nectar from a blossom.\",\n",
    "    \"A honeybee returned to the hive with pollen.\",\n",
    "    \n",
    "    \"A dove perched on the rooftop.\",\n",
    "    \"The pigeon cooed softly in the park.\",\n",
    "    \"A squab was learning to fly.\",\n",
    "    \n",
    "    \"A ladybug landed on my hand.\",\n",
    "    \"The ladybird crawled on a leaf.\",\n",
    "    \"A little beetle scurried across the petal.\"\n",
    "\n",
    "    \"A tree-dweller landed on my hand.\",\n",
    "    \"A little spotted bug across the petal.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n Testing model on various sentences...\\n\")\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        detected_animals = extract_animals(sentence, model, tokenizer)\n",
    "        print(f\"Text: {sentence}\")\n",
    "        print(f\"Detected Animals: {detected_animals}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "test_model_on_examples(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d640c-e7dd-4590-9a58-244fadc54ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
